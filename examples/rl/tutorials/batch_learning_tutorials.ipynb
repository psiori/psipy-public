{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Learning Tutorials\n",
    "This tutorial will explain how to train a controller to perform the batch and growing batch learning tasks.\n",
    "\n",
    "We will be using our simulated Cartpole for this Batch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "from psipy.rl.loop import Loop\n",
    "from psipy.rl.core.controller import DiscreteRandomActionController, ContinuousRandomActionController\n",
    "from psipy.rl.io.batch import Batch, Episode\n",
    "\n",
    "LOG = logging.getLogger(\"psipy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will test for the correct installation of the CartPole and pygame. You should see a pygame window popup and some (random) trajectories on the cartpole plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psipy.rl.plants.simulated.cartpole import (\n",
    "    CartPole,\n",
    "    CartPoleState,\n",
    "    CartPoleBangAction)\n",
    "\n",
    "plant = CartPole(start_angle=0,          # start upright (default: start hanging down)\n",
    "                 valid_angle=np.pi / 6)  # terminate episode and reset if pole falls below this angle.\n",
    "\n",
    "rc = DiscreteRandomActionController(state_channels=CartPoleBangAction.channels,\n",
    "                                    action=CartPoleBangAction)\n",
    "\n",
    "plant.notify_episode_starts()\n",
    "state = plant.check_initial_state(None)\n",
    "\n",
    "for _ in range(500):\n",
    "    action = rc.get_action(state)\n",
    "    state = plant.get_next_state(state, action)\n",
    "    plant.render()\n",
    "\n",
    "    if state.terminal:\n",
    "        plant.notify_episode_stops()\n",
    "        plant.reset()\n",
    "        plant.notify_episode_starts()\n",
    "        state = plant.check_initial_state(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "We use two learning paradigms, depending on the application and situation: batch and growing batch.\n",
    "### Batch\n",
    "In the batch paradigm, data is collected through some means, either a random controller, human, or otherwise.  A controller is trained on this data until convergence, and applied to the plant to test final performance.  In simple tasks, this should be sufficient, especially if the goal state or orther desirable regions of the state space can be reached by a random policy.\n",
    "<img src=\"batch-paradigm.png\">\n",
    "### Growing Batch\n",
    "In the growing batch paradigm, initial data can either be collected the same way as in the batch approach above. A controller is then trained on the intial batch for some iterations of the algorithm, and this controller is then used to collect *more* data from the plant.  The 'batch of data' grows, and the controller is trained on this bigger batch.  In this way, the controller essentially explores the state space itself, and can 'learn from its mistakes'.\n",
    "<img src=\"growing-paradigm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Tutorial\n",
    "It is best practice to lay out all your learning components for ease of use and reading.  Here, we define some placeholder variables for our plant, action, state, and lookback (how many observations are in a stack to create a state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_type = CartPoleState\n",
    "action_type= CartPoleBangAction\n",
    "\n",
    "lookback = 1   # no history because we have no latencies in the simulation\n",
    "sart_folder = \"psidata-tutorial-batch-sart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use NFQ to control this plant.  Therefore, we need a neural network internal model that we create below. We also select a subset of those channels that we want to use for the state_representaiton. Please note, these can be, and in the this case, are different from the channels used in the Cartpole internally. We use (sin, cos) to represent the pole angle, not the pole angle directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psipy.rl.controllers.nfq import NFQ\n",
    "\n",
    "state_channels = [\n",
    "    \"cart_position\",\n",
    "    \"cart_velocity\",\n",
    "    \"pole_sine\",\n",
    "    \"pole_cosine\",\n",
    "    \"pole_velocity\"]\n",
    "\n",
    "n_inputs = len(state_channels)\n",
    "n_outputs = len(action.legal_values[0])\n",
    "\n",
    "def make_model(n_inputs, n_outputs, lookback):\n",
    "    inp = tfkl.Input((n_inputs, lookback), name=\"state\")\n",
    "    net = tfkl.Flatten()(inp)\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(100, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(n_outputs, activation=\"sigmoid\")(net)\n",
    "    model = tf.keras.Model(inp, net)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = make_model(n_inputs=n_inputs, n_outputs=n_outputs, \n",
    "                   lookback=lookback)\n",
    "\n",
    "# Create the controller with our model.  \n",
    "controller = NFQ(model=model,\n",
    "                 state_channels=state_channels, \n",
    "                 action=action_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want a controller to explore.  We could use NFQ to explore, since with randomized weights it essentially acts as a random controller, but we will explicitly use a random controller here to demonstrate how to use different controllers at the same time. Since `CartPoleAction` is discrete, we use a `DiscreteRandomActionController`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = DiscreteRandomActionController(state_channels=state.channels(), action=action_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `Loop`, which takes a name (the name in the SART logs), a plant, a controller, and a path to save the SART logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = Loop(plant, explorer, \"CartPole\", sart_folder, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now collect some data with our explorer, the `DiscreteRandomActionController`. We want to collect 100 episodes.\n",
    "**Attention:** Please ignore possible \"ValueErrors\" thrown by hdf5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loop.run(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that if you run this notebook or this cell multiple times, old data collected from previous runs will also be loaded unless you have deleted the SART folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data into a `Batch` from the hdf5 files we just created.  Be aware that you have to set the lookback as well as the used state channels here! We also have to load the action's index (move_index) not its actual value (move), as this variant of NFQ uses a DQN-like network topology where for each of the possible actions, it has one output neuron that will be trained on that particular actions q-value. Thus, we need the index of the selected action, that corresponds to the index of the output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch.from_hdf5(sart_folder, lookback=lookback, state_channels=state_channels, control=controller, action_channels=[\"move_index\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note the logs: at the bottom, the logs will always tell you how many episodes were loaded.  If you want to know at any other point how many episodes the batch has loaded, check the `num_episodes` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks like normalized data, so we fit NFQ's normalizer on the observations in the batch.  We have to pass in the batch's observations to fit the normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit_normalizer(batch.observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for fitting.  We pass in the batch, and train.  This will take a couple minutes (not too long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "controller.fit(\n",
    "    batch,\n",
    "    iterations=20,\n",
    "    epochs=2,\n",
    "    minibatch_size=50,\n",
    "    gamma=0.98,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! \n",
    "\n",
    "Sort of.\n",
    "\n",
    "Now we can see how our trained controller fairs live in the plant.  Let's run the controller again by creating a new `Loop`, but this time not store the data in our SART folder (otherwise if we want to train again we will train on this data as well, i.e. growing batch).  We do this by changing the `logdir` param to something different from our SART folder.  I prefer prepending \"live-\" to the SART folder name.  Finally, we render the environment so we can see what is happening.  Enjoy!\n",
    "\n",
    "**It's likely your controller will still fail and not balance the pole indefinetely.** You could try and collect more data and run NFQ for more iterations. Or you could read ahead how to make this process more (data) efficient.\n",
    "\n",
    "*Note: the environment will not close on its own. We know of this issue and won't fix it (yet) :D*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loop = Loop(plant, controller, \"CartPoleEval\", f\"{sart_folder}-evaluation\", render=True)\n",
    "eval_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing Batch Example\n",
    "\n",
    "The following will grow a batch of data interleaved with improving the model. Note, that we now use the controller instead of the explorer in the loop. We use the same sart_folder, adding additional data to the already existing data, which has been random collected. More data is never bad (as we use an off-policy method), no reason to start from fresh.\n",
    "\n",
    "This training will take a while due to the network training inbetween the collection of episodes of interaction with the cartople. It should learn to balance the pole within 20 cycles of the outer loop or less. If it's not perfect, just execute the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loop = Loop(plant, controller, \"CartPole\", sart_folder, render=True)\n",
    "controller.epsilon = 0.1   # try a random action every 20 steps for continuous exploration of alternatives.\n",
    "\n",
    "for cycle in range(20):\n",
    "    print(f\"Cycle: {cycle}\")\n",
    "\n",
    "    # execute two explorative runs\n",
    "    loop.run(2, max_episode_steps=400)\n",
    "\n",
    "    # load the grown batch\n",
    "    batch = Batch.from_hdf5(sart_folder,\n",
    "                            lookback=lookback,\n",
    "                            state_channels=state_channels,\n",
    "                            control=controller,\n",
    "                            action_channels=[\"move_index\",])\n",
    "    \n",
    "    # fit the controller for another few iterations\n",
    "    controller.fit(\n",
    "        batch,\n",
    "        iterations=4,\n",
    "        epochs=8,\n",
    "        minibatch_size=500,\n",
    "        gamma=0.98\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cycles = 10\n",
    "\n",
    "loop = Loop(plant, controller, \"GymCartPole\", sart_folder, render=True)\n",
    "\n",
    "for cycle in range(num_cycles):\n",
    "    loop.run(5)\n",
    "\n",
    "    batch.append_from_hdf5(sart_folder, action_channels=[\"move_index\",])\n",
    "    print(f\"Current batch size: {batch.num_episodes}\")\n",
    "\n",
    "    controller.fit(\n",
    "        batch,\n",
    "        iterations=20,\n",
    "        epochs=25,\n",
    "        minibatch_size=64,\n",
    "        gamma=0.99,\n",
    "        verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** everything below has not yet been adapted to psipy-public and will not work right now. WIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing Batch Tutorial with Continuous Actions\n",
    "\n",
    "We will use the the same plant here, but let it start with the pole hanging, trying to learn to swing it up and balance it. We will also use NFQ-CA, that is an actor-critic variant of NFQ with continuous actions.\n",
    "\n",
    "As with the Batch tutorial, we will lay out all of our learning components for ease of use and reading.  Here, we define some placeholder variables for our plant, action, state, and lookback (how many observations are in a stack to create a state). Some is copied from above, just for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psipy.rl.plants.simulated.cartpole import (\n",
    "    CartPoleContinuousAction,\n",
    ")\n",
    "action_type = CartPoleContinuousAction\n",
    "state_type = CartPoleState\n",
    "\n",
    "lookback = 1\n",
    "sart_folder = \"psidata-growing-sart\"\n",
    "\n",
    "plant = CartPole(\n",
    "    action_type=CartPoleContinuousAction\n",
    ")  # Note that it is instantiated!\n",
    "\n",
    "state_channels = [\n",
    "    \"cart_position\",\n",
    "    \"cart_velocity\",\n",
    "    \"pole_sine\",\n",
    "    \"pole_cosine\",\n",
    "    \"pole_velocity\"]\n",
    "\n",
    "n_inputs = len(state_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use NFQCA to control this plant.  Therefore, we need a two neural network internal models, one for the actor and one for the critic.  Below we use functions to create the models, but this is not necessary.  Note that we need to use the lookback to properly shape our neural networks' inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psipy.rl.controllers.nfqca import NFQCA\n",
    "from psipy.rl.controllers.noise import RandomNormalNoise\n",
    "\n",
    "\n",
    "def make_actor(inputs, lookback):\n",
    "    inp = tfkl.Input((inputs, lookback), name=\"state_actor\")\n",
    "    net = tfkl.Flatten()(inp)\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(100, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(1, activation=\"tanh\")(net)\n",
    "    return tf.keras.Model(inp, net, name=\"actor\")\n",
    "\n",
    "\n",
    "def make_critic(inputs, lookback):\n",
    "    inp = tfkl.Input((inputs, lookback), name=\"state_critic\")\n",
    "    act = tfkl.Input((1,), name=\"act_in\")\n",
    "    net = tfkl.Concatenate()([tfkl.Flatten()(inp), tfkl.Flatten()(act)])\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(256, activation=\"relu\")(net)\n",
    "    net = tfkl.Dense(100, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(1, activation=\"sigmoid\")(net)\n",
    "    return tf.keras.Model([inp, act], net, name=\"critic\")\n",
    "\n",
    "actor = make_actor(n_inputs, lookback)\n",
    "critic = make_critic(n_inputs, lookback)\n",
    "\n",
    "controller = NFQCA(\n",
    "    actor=actor, \n",
    "    critic=critic, \n",
    "    state_channels=state_channels, \n",
    "    action=action_type,\n",
    "    lookback=lookback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `Loop`, which takes a name (the name in the SART logs), a plant, a controller, and a path to save the SART logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = Loop(plant, controller, \"CartpoleSwingup\", sart_folder, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use NFQCA to do the initial exploration, as well as all data collection.  We first collect some initial data outside the loop and then collect more data within the growing-batch-loop.  We print some extra things so you can see what is going on, but that is unnecessary.\n",
    "\n",
    "Note that depending on the internal cost function of the `CartPoleSwayEnv` at this time, NFQCA might learn nothing.  This tutorial just shows the general form of training NFQCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 400\n",
    "steps = 400\n",
    "cycles = 2\n",
    "\n",
    "controller.exploration=RandomNormalNoise(size=1, std=1.0)  # this achieves exploration by adding noise to all actions\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    loop.run(1, max_episode_steps=steps)\n",
    "    \n",
    "    batch = Batch.from_hdf5(sart_folder, lookback=lookback, control=controller, state_channels=state_channels)\n",
    "\n",
    "    for cycle in range(cycles):\n",
    "\n",
    "        print(f\"Iteraton {episode}/{cycle} with batch size: {batch.num_episodes}\")\n",
    "\n",
    "        if episode < 20 or (episode < episodes / 2 and episode % 20 == 0):\n",
    "            controller.fit_normalizer(batch.observations, method=\"meanstd\")\n",
    "\n",
    "        # NFQCA does not have a generic fit method\n",
    "        controller.fit_critic(\n",
    "            batch,\n",
    "            iterations=2,\n",
    "            epochs=8,\n",
    "            minibatch_size=8192,\n",
    "            gamma=0.98,\n",
    "            verbose=0)\n",
    "        \n",
    "        controller.fit_actor(\n",
    "            batch,\n",
    "            epochs=1,\n",
    "            minibatch_size=2048,\n",
    "            verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how the model performs live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = Loop(plant, controller, \"GrowingBatchEval\", f\"live-{sart_folder}\", render=True)\n",
    "loop.run(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've made it to the next level! Congratulations.  Now it is time to delve deeper into the power of offline reinforcement learning.  The general guidelines training will not be outlined here.\n",
    "\n",
    "Let's train `CartPoleSway` again, but this time alter the cost function and create fake transitions to aid the cart to its goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Setting\n",
    "We want the cart to move the position 0 (middle of the screen) from any starting position.  We do not care about the pole.\n",
    "\n",
    "For this, we will generate a cost function that only deals out cost based on position, and create a fake transition set that shows 0 cost at the goal position.\n",
    "\n",
    "*Note the imports inside the functions: this is bad practice but I do it here to show what imports we need.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_episodes(sart_path:str, lookback:int): # ->List[Episode]\n",
    "    \"\"\"Create a fake episode at position 0 for every episode already collected\n",
    "    \n",
    "    Since the cart probably was never at position 0 exactly, we add a set of Episodes with this transition.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    from psipy.rl.io.sart import SARTReader\n",
    "    from psipy.rl.io.batch import Episode\n",
    "    \n",
    "    more_episodes = []\n",
    "\n",
    "    for path in glob.glob(f\"{sart_path}/*.hdf5\"):\n",
    "        with SARTReader(path) as reader:\n",
    "            o, a, t, c = reader.load_full_episode()\n",
    "\n",
    "            # Add episode full of goal states\n",
    "            o = o.copy()\n",
    "            a = a.copy()\n",
    "            o[:, 0] = 0\n",
    "            o[:, 1] = 0\n",
    "            # A swinging pole affects the position of the cart,\n",
    "            # so we say no swing here as well\n",
    "            o[:, 2] = 180\n",
    "            o[:, 3] = 0\n",
    "            a[:] = 0  # The cart should not move once in this position\n",
    "            more_episodes.append(Episode(o, a, t, c, lookback=lookback))\n",
    "            \n",
    "    return more_episodes\n",
    "\n",
    "def costfunc(states:np.ndarray): # -> np.ndarray\n",
    "    \"\"\"Recalculate costs on all states provided\n",
    "    \n",
    "    This calculates costs on multiple states, so it returns an array\n",
    "    \"\"\"\n",
    "    from psipy.rl.control.nfq import tanh2\n",
    "    # Position is already defined against 0 (relative)\n",
    "    position = states[:, 0]\n",
    "    cost = tanh2(position, C=.2, mu=.1)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set everything up until we need to add the fake transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psipy.rl.plant.gym.cartpole_plants import (\n",
    "    CartPoleSwayContAction,\n",
    "    CartPoleSwayContinuousPlant,\n",
    "    CartPoleSwayState,\n",
    ")\n",
    "from psipy.rl.control.nfqca import NFQCA\n",
    "\n",
    "\n",
    "plant = CartPoleSwayContinuousPlant()  # Note that it is instantiated!\n",
    "action = CartPoleSwayContAction\n",
    "state = CartPoleSwayState\n",
    "lookback = 5\n",
    "sart_folder = \"tutorial-advanced-sart\"\n",
    "\n",
    "\n",
    "def make_actor(inputs, lookback):\n",
    "    inp = tfkl.Input((inputs, lookback), name=\"state_actor\")\n",
    "    net = tfkl.Flatten()(inp)\n",
    "    net = tfkl.Dense(40, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(40, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(1, activation=\"tanh\")(net)\n",
    "    return tf.keras.Model(inp, net, name=\"actor\")\n",
    "\n",
    "\n",
    "def make_critic(inputs, lookback):\n",
    "    inp = tfkl.Input((inputs, lookback), name=\"state_critic\")\n",
    "    act = tfkl.Input((1,), name=\"act_in\")\n",
    "    net = tfkl.Concatenate()([tfkl.Flatten()(inp), tfkl.Flatten()(act)])\n",
    "    net = tfkl.Dense(40, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(40, activation=\"tanh\")(net)\n",
    "    net = tfkl.Dense(1, activation=\"sigmoid\")(net)\n",
    "    return tf.keras.Model([inp, act], net, name=\"critic\")\n",
    "\n",
    "actor = make_actor(len(state.channels()), lookback)\n",
    "critic = make_critic(len(state.channels()), lookback)\n",
    "\n",
    "controller = NFQCA(\n",
    "    actor=actor, \n",
    "    critic=critic, \n",
    "    state_channels=state.channels(), \n",
    "    action=CartPoleSwayContAction,\n",
    "    lookback=lookback\n",
    ")\n",
    "\n",
    "loop = Loop(plant, controller, \"CartPolePosition\", sart_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critic recieves the cost function, and before we start training we want to add our fake goal position transitions.  Therefore, we put `costfunc` in `fit_critic` and append fake episodes before we start the fitting cycles.\n",
    "\n",
    "We will print the size of the batch so it can be seen how the fake episodes increase the batch episode count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cycles = 3\n",
    "iterations = 3\n",
    "\n",
    "loop.run(10)\n",
    "batch = Batch.from_hdf5(sart_folder, lookback=lookback, control=controller)\n",
    "print(f\"Current batch size: {batch.num_episodes}\")\n",
    "# We now append the fake created episodes\n",
    "# We could also throw away the created batch and only have the episodes\n",
    "# created in the function by doing:\n",
    "# batch = Batch(create_fake_episodes(sart_folder, lookback))\n",
    "batch = batch.append(create_fake_episodes(sart_folder, lookback))\n",
    "        \n",
    "for cycle in range(num_cycles):\n",
    "    LOG.info(\"Cycle: %d\", cycle + 1)\n",
    "    print(f\"Current batch size: {batch.num_episodes}\")\n",
    "    controller.fit_normalizer(batch.observations, method=\"meanstd\")\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        LOG.info(\"NFQCA Iteration: %d\", iteration + 1)\n",
    "        controller.fit_critic(batch,\n",
    "                         iterations=1,\n",
    "                         # We add the cost function here\n",
    "                         costfunc=costfunc,\n",
    "                         epochs=10,\n",
    "                         minibatch_size=-1,\n",
    "                         gamma=1.0,\n",
    "                         verbose=0)\n",
    "        controller.fit_actor(batch,\n",
    "                        epochs=10,\n",
    "                        minibatch_size=-1,\n",
    "                        verbose=0)\n",
    "\n",
    "    loop = Loop(plant, controller, \"GrowingBatch\", sart_folder)\n",
    "    loop.run(10)\n",
    "    batch.append_from_hdf5(sart_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the controller moves to the goal position.  Oh the tension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = Loop(plant, controller, \"GrowingBatchEval\", f\"live-{sart_folder}\", render=True)\n",
    "loop.run(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've made it through the Advanced Tutorial.  You are now an expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please delete the SART logs created by this tutorial if you no longer need them.  Or run the cell below to do that automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "dirs = [\"tutorial-advanced-sart\", \n",
    "        \"tutorial-batch-sart\", \n",
    "        \"tutorial-growing-batch\", \n",
    "        \"live-tutorial-growing-sart\", \n",
    "        \"live-tutorial-batch-sart\", \n",
    "        \"live-tutorial-advanced-sart\"]\n",
    "for d in dirs:\n",
    "    try:\n",
    "        shutil.rmtree(d)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
