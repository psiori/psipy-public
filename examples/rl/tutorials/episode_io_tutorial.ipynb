{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SART Writer and Reader Tutorial\n",
    "This tutorial will explain how to write SART (State, Action, Reward, Terminal) from episodes to hdf5 files, and read them back out.\n",
    "## Sart Writer Tutorial\n",
    "The SART Writer runs in a separate thread and takes SART tuples from a queue and writes them to hdf5 files.  One file per episode is written.  The files are saved with the following naming scheme:\n",
    "\n",
    "             filepath/sart-name-episode-timestamp.hdf5\n",
    "            \n",
    "where name is a custom name given to tell what it is, episode is the episode number, and timestamp is the time in format ```'%Y%m%d-%H%M%S'``` at the creation of the file (not at close!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, time\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from psipy.rl.plant import Action, State, Plant\n",
    "from psipy.rl.control.controller import Controller\n",
    "from psipy.rl.control.controller import DiscreteRandomActionController\n",
    "from psipy.rl.io.sart import SARTWriter, SARTReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The writer takes a *directory* path, i.e., this directory will receive the file.  \n",
    "\n",
    "The ```buffer_size``` parameter controls how big a resizable dataset inside the hdf5 file can get before increasing in size. This number shoud be tuned based on how big episodes usually are.  Too small and the writer will have to resize often; too big isn't much of a problem besides being inefficient.  At file close time, the excess \"size\" is trimmed off the datasets so that they only contain the data put into them (unused portions of the data are filled with np.nan).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate a fake plant and other objects in order to demo the writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestState(State):\n",
    "    _channels = (\"sensor1\", \"sensor2\")\n",
    "\n",
    "class TestAction(Action):\n",
    "    dtype = \"discrete\"\n",
    "    num_values = 1\n",
    "    channels = (\"act1\", \"act1/2\")  # / will transform to | (pipe) since it conflicts with hdf5 group paths!\n",
    "    legal_values = (range(101), range(101))\n",
    "\n",
    "class TestPlant(Plant):\n",
    "    state_type = TestState\n",
    "    action_type = TestAction\n",
    "\n",
    "    def check_initial_state(self, state: State) -> State:\n",
    "        return self._get_next_state(TestState({\"sensor1\": 10, \"sensor2\":20}), None)\n",
    "\n",
    "    def _get_next_state(self, state: State, action: Action) -> State:\n",
    "        state.terminal = False\n",
    "        state.reward = 1\n",
    "        return TestState({\"sensor1\": 10, \"sensor2\": 20})\n",
    "\n",
    "    def notify_episode_stops(self) -> bool:\n",
    "        pass\n",
    "\n",
    "plant = TestPlant()\n",
    "name = \"TutorialRun\"\n",
    "logdir = os.path.join(\".\", \"tutorial-sart-logs\")\n",
    "max_steps = 15  # just how many loops we will do in the \"episode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the writer.  We set the buffer_size here to 5 so that we can see what happens when the buffer resizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sart_writer = SARTWriter(logdir, name, episode=1, buffer_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simulate a loop during an episode.  At the end of each step, the state, action, and meta information is appended to the writer as a dict of dicts.\n",
    "\n",
    "At the end of the episode, the writer is notified to close via ```.notify_episode_stops()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant.notify_episode_starts()\n",
    "state = plant.check_initial_state(None)\n",
    "\n",
    "drc = DiscreteRandomActionController(TestState.channels(), TestAction)\n",
    "\n",
    "for steps in range(1, max_steps + 1):\n",
    "    action = drc.get_action(state)\n",
    "    next_state = plant.get_next_state(state, action)\n",
    "    reward = plant.get_cost(next_state)\n",
    "    terminal = plant.is_terminal(next_state)\n",
    "\n",
    "    sart_writer.append(\n",
    "        {\n",
    "            \"state\": state.as_dict(),\n",
    "            \"action\": action.as_dict(),\n",
    "            \"meta\": OrderedDict(meta=OrderedDict(world=43110)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "plant.notify_episode_stops()\n",
    "sart_writer.notify_episode_stops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hdf5 file is written, let's quickly look inside manually to see the structure before we use the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['action', 'state']>\n",
      "<KeysViewHDF5 ['cost', 'terminal', 'values']>\n",
      "<KeysViewHDF5 ['sensor1', 'sensor2']>\n",
      "[20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n",
      "(15,)\n",
      "[14. 62. 30. 58. 69. 75.  1. 70. 16. 23. 88.  6. 26. 55. 37.]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "filename = os.listdir(\"tutorial-sart-logs/\")[0]\n",
    "f = h5py.File(os.path.join(\"tutorial-sart-logs/\", filename), 'r')\n",
    "print(f.keys())\n",
    "print(f['state'].keys())\n",
    "print(f['state']['values'].keys())\n",
    "print(f['state']['values']['sensor2'][:])\n",
    "print(f['state']['values']['sensor2'][:].shape)\n",
    "print(f['action']['act1'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the state, action, and meta channels are saved as bytes in the file's attributes, so that a ```state```/```action```/```meta``` object can be recreated when loading again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sensor1' 'sensor2']\n",
      "['act1' 'act1/2']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f.attrs['state'][:])\n",
    "print(f.attrs['action'][:])\n",
    "print(f.attrs['meta'][:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sart Reader Tutorial\n",
    "The SART Reader loads a single episode file at a time.  Provide it simply the filepath to the hdf5 file.  Once created, it will read the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SARTReader(os.path.join('tutorial-sart-logs', filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, only extracting the full episode is possible.  Full episode loading extracts the data into a tuple in the format expected by the ```Episode``` class, i.e. (observations, actions, terminals, costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.],\n",
       "        [10., 20.]], dtype=float32),\n",
       " array([[14., 50.],\n",
       "        [62., 70.],\n",
       "        [30., 17.],\n",
       "        [58., 95.],\n",
       "        [69., 82.],\n",
       "        [75., 28.],\n",
       "        [ 1., 31.],\n",
       "        [70., 21.],\n",
       "        [16., 30.],\n",
       "        [23., 10.],\n",
       "        [88., 54.],\n",
       "        [ 6., 48.],\n",
       "        [26., 57.],\n",
       "        [55., 97.],\n",
       "        [37., 68.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.load_full_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done for multiple files via the ```Batch``` class's classmethod, ```from_hdf5```.  Feel free to run the writing section multiple times to generate more hdf5 files to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slange/Code/sabbatical/dm-psiori/psipy/psipy/rl/io/batch.py:71: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  self._terminals = np.asarray(terminals, dtype=np.bool).ravel()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpsipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch\n\u001b[0;32m----> 3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtutorial-sart-logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/Code/sabbatical/dm-psiori/psipy/psipy/rl/io/batch.py:849\u001b[0m, in \u001b[0;36mBatch.from_hdf5\u001b[0;34m(cls, lookback, state_channels, action_channels, prioritization, control, only_newest, override_mtime, *dirpaths)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hdf5_file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m         eps \u001b[38;5;241m=\u001b[39m \u001b[43mEpisode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhdf5_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    856\u001b[0m         LOG\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhdf5_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/sabbatical/dm-psiori/psipy/psipy/rl/io/batch.py:217\u001b[0m, in \u001b[0;36mEpisode.from_hdf5\u001b[0;34m(cls, filepath, lookback, state_channels, action_channels)\u001b[0m\n\u001b[1;32m    215\u001b[0m o, a, t, c \u001b[38;5;241m=\u001b[39m episode\n\u001b[1;32m    216\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mremove_string_axes(a)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/sabbatical/dm-psiori/psipy/psipy/rl/io/batch.py:71\u001b[0m, in \u001b[0;36mEpisode.__init__\u001b[0;34m(self, observations, actions, terminals, costs, lookback)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(terminals, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_costs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(costs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookback \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Code/sabbatical/dm-psiori/.venv/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from psipy.rl.io.batch import Batch\n",
    "\n",
    "batch = Batch.from_hdf5(os.path.join('tutorial-sart-logs'), \n",
    "                        lookback=5)\n",
    "\n",
    "print()\n",
    "print(batch)\n",
    "print(batch.nextstates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWMR\n",
    "SART Writer versions 2.0.0 and greater have \"single write, multiple read\" (SWMR) mode activated.  That means that the SART Reader can read a SART file while it is being written.  This is useful for dashboards and the like which want to display data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to remove the sart data written during the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('tutorial-sart-logs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
