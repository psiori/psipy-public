{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPROP Demo\n",
    "This notebook demonstrates the usage of The RPROP optimizer on the example of the Autoencoder and a simple network trained on the Fashion MNIST dataset.\n",
    "The RPROP optimizer optimizes the parameters based on the direction of the gradient, but not its magnitude. In each iteration it goes one step in the opposite direction of the gradient.The stepsize is small in the beginning and is increased in each iteration until the gradient changes its direction, which means the Minimum was crossed. Then it goes back the last step and the stepsize is decreased for the next step. Then the stepsize is increased again until the gradient changes its sign again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from scipy import interpolate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import the RPROP optimizer\n",
    "from psipy.nn.optimizers.rprop import RpropPlus, iRpropPlus\n",
    "\n",
    "# import the autoeconder.\n",
    "from psipy.dataroom.internal.autoencoder import FullyConnectedAutoencoder\n",
    "\n",
    "# Set the random seed.\n",
    "from numpy.random import seed\n",
    "seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Fashion MNIST dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data must be preprocessed before training the network. The pixel values fall in the range of 0 to 255. \n",
    "# Scale values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values \n",
    "# by 255.\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Use RpropPlus() or iRpropPlus() as optimizer in the model\n",
    "model.compile(optimizer=RpropPlus(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train & evaluate the model with full batch_size\n",
    "model.fit(train_images, train_labels, batch_size=len(train_images), epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with same model, but Adam optimizer and minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model \n",
    "model2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Use Adam as optimizer in the model\n",
    "model2.compile(optimizer='Adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train & evaluate the model with full batch_size\n",
    "model2.fit(train_images, train_labels, epochs=10)\n",
    "test_loss2, test_acc2 = model2.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is completely copied from the Autoencoer Demo notebook. For detailed explanation, look at its comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for data generation.\n",
    "def cubeND(n):\n",
    "    \"\"\"Returns waypoints for non-intersecting path connecting the corners of an n-D cube\"\"\"\n",
    "    pattern = [0, 1, 1, 0]\n",
    "    c = np.zeros((2**n+1, n))\n",
    "    for d in range(n-1):\n",
    "        repeated_pattern = np.repeat(pattern, 2**d)\n",
    "        repeated_tiled_pattern = np.tile(repeated_pattern, 2**(n-d-2))\n",
    "        c[:-1,d] = repeated_tiled_pattern\n",
    "    c[:-1,-1] = np.repeat(pattern, 2**(n-1))[:2**n]\n",
    "    c[2**n,:] = c[0,:]\n",
    "    return c\n",
    "\n",
    "def dataCubeND(n, step=0.01, std=0.05):\n",
    "    \"\"\"Returns data points with added noise along path through n-D cube\"\"\"\n",
    "    c = cubeND(n)\n",
    "    interp = interpolate.interp1d(range(len(c)), c, axis=0)\n",
    "    d = interp(np.arange(0, len(c)-1+step, step))\n",
    "    return d + np.random.standard_normal(d.shape)*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataCubeND(3, 0.01, 0.01) - 0.5\n",
    "X_train, X_test = train_test_split(data)\n",
    "topology = [3, 6, 2]\n",
    "activation = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the model.\n",
    "# Use RpropPlus() or iRpropPlus() as optimizer in the model\n",
    "ae = FullyConnectedAutoencoder(optimizer=RpropPlus() ,topology=topology, \n",
    "                               hidden_layer_kwargs=dict(activation=activation, kernel_initializer='random_normal'),\n",
    "                               code_layer_kwargs=dict(activation=activation, kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We start by first pretraining the network. Instead of training the whole network, pretraining starts by training\n",
    "# individual layers, adding layers gradually and retraining until all the network layers have been added.\n",
    "# It acts as a 'warm up' for the actual training, as our network already has some sense of 'direction'.\n",
    "# We do the training for the set number of epochs, where in each epoch we feed each of the training \n",
    "# sample in X_train to the network.\n",
    "# In contrast to the standard optimizer used in Autoencoder Demo notebook, the RPROP optimizer needs no \n",
    "# learning rate lr.\n",
    "# The RPROP optimizer works on the full batch size. If it is not specified the default batch size would be 32.\n",
    "\n",
    "\n",
    "val_error = ae.pretrain(X_train, validation_data=(X_test, X_test), batch_size=len(X_train), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_error = ae.fit(X_train, validation_data=(X_test, X_test), batch_size=len(X_train), epochs=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "In the following ther eis some evaluation of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to the learned lower dimensional embedding.\n",
    "embedding = ae.transform(data)\n",
    "\n",
    "# Try to reconstruct the original higher dimensional data from the learned lower dimensional embedding.\n",
    "rec = ae.inverse_transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now plot the orginal data, the learned lower embedding and the data\n",
    "# reconstructed from only the lower dimensional embedding. \n",
    "# The hope is that the original data and reconstructed data look very similar.\n",
    "# And by looking at the learned embedding one could also get some insight in to what \n",
    "# the network learns.\n",
    "\n",
    "\n",
    "cm = plt.cm.get_cmap('RdYlGn')\n",
    "fig = plt.figure(figsize=(4.5,12))\n",
    "\n",
    "\n",
    "# Plot the original data.\n",
    "ax = fig.add_subplot(311, projection='3d')\n",
    "ax.scatter(data[:,0], data[:,1], zs=data[:,2], c=np.arange(len(data)), cmap=cm)\n",
    "plt.title('Original')\n",
    "\n",
    "\n",
    "# Plot the learned lower dimensional embedding.\n",
    "ax = fig.add_subplot(312)\n",
    "ax.scatter(embedding[:,0], embedding[:,1], c=np.arange(len(data)), cmap=cm)\n",
    "plt.title('Embedding')\n",
    "\n",
    "\n",
    "# Plot the image reconstructed using the learned lower dimensional embedding.\n",
    "ax = fig.add_subplot(313, projection='3d')\n",
    "ax.scatter(rec[:, 0], rec[:, 1], zs=rec[:, 2], c=np.arange(len(data)), cmap=cm)\n",
    "plt.title('Reconstruction')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
